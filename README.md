# ğŸ§° ImplementaciÃ³n de HAProxy + Keepalived (Ansible)

---

## ğŸ§± DescripciÃ³n: Â¿QuÃ© estÃ¡s construyendo?

EstÃ¡s construyendo un entorno de Kubernetes de alta disponibilidad en mÃ¡quinas virtuales KVM hospedadas en un servidor fÃ­sico HP ProLiant. La configuraciÃ³n incluye K3s, HAProxy + Keepalived, Traefik como Ingress interno, almacenamiento distribuido con Longhorn + NFS, todo asegurado con WireGuard VPN y nftables.

---

## ğŸŒ Arquitectura de Red y Acceso Externo

```plaintext
[Usuarios PÃºblicos]
       â”‚
       â–¼
+-------------------+
| Cloudflare CDN    | â—„â”€â”€ Proxy + HTTPS + WAF
| (example.com)     |
+-------------------+
       â”‚
       â–¼
IP dinÃ¡mica DNS (example.com) API Cloudflare script bash actualizar la IP
       â”‚
       â–¼
+-----------------------------+
| Servidor WireGuard LAN       |
| NAT + VPN (192.168.0.40)     |
+-----------------------------+
       â”‚
       â–¼
TrÃ¡fico Interno Redirigido segÃºn Tipo
ğŸ¯ SegmentaciÃ³n del TrÃ¡fico en ProducciÃ³n
Tipo de TrÃ¡fico	VIP Asignada	FunciÃ³n
Kubernetes API (6443)	192.168.0.32	Asegura estabilidad para kubectl, etcd, control-plane
Ingress HTTP/HTTPS	192.168.0.33	Redirige el trÃ¡fico hacia servicios internos vÃ­a Traefik
```

Estas IPs Virtuales (VIPs) son gestionadas por HAProxy + Keepalived y cambian automÃ¡ticamente entre nodos.

---

## ğŸ§  Balanceadores de Carga de Alta Disponibilidad

```plaintext
Nodo	IP	FunciÃ³n
loadbalancer1	192.168.0.30	HAProxy master + VIP
loadbalancer2	192.168.0.31	HAProxy backup + VIP
```

Los tres nodos tienen HAProxy + Keepalived instalados. Las VIPs 192.168.0.32 (API) y 192.168.0.33 (Ingress) son flotantes, con solo un nodo manteniÃ©ndolas en cada momento segÃºn la prioridad.

---

## â˜¸ï¸ ClÃºster de Kubernetes (K3s HA)

```plaintext
Hostname	IP	Rol
master1	10.17.4.21	etcd + API
master2	10.17.4.22	etcd
master3	10.17.4.23	etcd
worker1	10.17.4.24	Nodo de aplicaciÃ³n
worker2	10.17.4.25	Nodo de aplicaciÃ³n
worker3	10.17.4.26	Nodo de aplicaciÃ³n
```

Todos los nodos usan Flatcar Container Linux. El clÃºster K3s opera en modo etcd HA. Se instala con `--tls-san 192.168.0.32` para permitir acceso a kubectl vÃ­a la VIP.

---

## ğŸšª Controlador de Ingress (Traefik)

```plaintext
Tipo	ImplementaciÃ³n
Traefik	ImplementaciÃ³n interna en Kubernetes
```

El acceso se realiza a travÃ©s de la VIP 192.168.0.33 gestionada por HAProxy. Traefik se comunica con los pods a travÃ©s de ClusterIP y no estÃ¡ expuesto directamente.

---

## ğŸ’¾ Almacenamiento Persistente

```plaintext
Nodo	IP	Rol
storage1	10.17.4.27	NFS + Longhorn
```

Longhorn (RWO):

- Microservicios
- Prometheus
- Grafana
- ELK

NFS (RWX):

- PostgreSQL â†’ `/srv/nfs/postgresql`
- Datos compartidos â†’ `/srv/nfs/shared`

---

## ğŸ” Seguridad

```plaintext
Componente	Detalles
WireGuard	Acceso remoto seguro desde el VPS
nftables	Cortafuegos estricto en el servidor fÃ­sico
Cloudflare	HTTPS, WAF, ProtecciÃ³n DDoS
AutenticaciÃ³n	basicAuth para dashboards internos
DNS/NTP	Intra-clÃºster (10.17.3.11)
```

---

## ğŸ§  AutomatizaciÃ³n y CI/CD

```plaintext
Herramienta	FunciÃ³n
Terraform	Aprovisionamiento de VM y red
Ansible	InstalaciÃ³n y configuraciÃ³n (100% IaC)
Jenkins + ArgoCD	CI/CD interno
```

---

## ğŸ–¥ Tabla de MÃ¡quinas

```plaintext
Hostname	IP	FunciÃ³n
master1	10.17.4.21	K3s Master + etcd
master2	10.17.4.22	K3s Master + etcd
master3	10.17.4.23	K3s Master + etcd
worker1	10.17.4.24	Nodo de aplicaciÃ³n
worker2	10.17.4.25	Nodo de aplicaciÃ³n
worker3	10.17.4.26	Nodo de aplicaciÃ³n
storage1	10.17.4.27	Longhorn + NFS
loadbalancer1	192.168.0.30	HAProxy + Keepalived (VIPs)
loadbalancer2	192.168.0.31	HAProxy (backup)
postgresql1	10.17.3.14	PostgreSQL centralizado
infra-cluster	10.17.3.11	CoreDNS + Chrony
```

---

## âœ… Ventajas de Esta Arquitectura

- Alta disponibilidad real con mÃºltiples VIPs separadas.
- Ingress controlado internamente con Traefik.
- Seguridad robusta a travÃ©s de VPN, nftables y HTTPS.
- AutomatizaciÃ³n completa (Terraform + Ansible).
- Almacenamiento distribuido y tolerante a fallos.
- DiseÃ±o modular para crecimiento sin rediseÃ±o.

---

## ğŸ§° DocumentaciÃ³n: Arranque del ClÃºster K3s con HAProxy + Keepalived + VIPs

### ğŸ“„ Objetivo

Permitir que el nodo master1 arranque el clÃºster K3s sin requerir que HAProxy o Keepalived estÃ©n activos y funcionales previamente. Esto resuelve el clÃ¡sico problema de dependencia cÃ­clica ("el huevo o la gallina") cuando se usa una VIP (IP Virtual) como punto de entrada al clÃºster.

### ğŸ›ï¸ Arquitectura

```plaintext
API Server VIP: 192.168.0.32
Ingress VIP: 192.168.0.33

Masters:
10.17.4.21 (bootstrap)
10.17.4.22
10.17.4.23

Workers:
10.17.4.24, 10.17.4.25, 10.17.4.26, 10.17.4.27

Load Balancers:
192.168.0.30, 192.168.0.31
```

---

### ğŸ”„ Orden de Inicio Esperado

1. El nodo master1 se inicializa con su IP real (10.17.4.21).
2. Se levanta el k3s-server y el etcd en master1.
3. Los otros masters se unen usando `https://10.17.4.21:6443` (no la VIP).
4. Una vez el clÃºster estÃ¡ operativo:
   - Se configura la VIP (192.168.0.32) con Keepalived.
   - Se habilita HAProxy en los nodos haproxy_keepalived.
   - HAProxy redirige el trÃ¡fico de `192.168.0.32:6443` hacia los masters disponibles.
   - El kubeconfig puede comenzar a usar la VIP como endpoint oficial.

---

### âœ… ConfiguraciÃ³n Correcta para Romper el Ciclo

1. **master1 usa su IP real para bootstrap**

   - El script de Ansible no apunta a la VIP (192.168.0.32) para levantar el nodo inicial.
   - Esto permite iniciar el API Server antes que HAProxy.

2. **HAProxy permite bind en IPs no locales**

```yaml
- name: Habilitar net.ipv4.ip_nonlocal_bind
  ansible.posix.sysctl:
    name: net.ipv4.ip_nonlocal_bind
    value: "1"
    sysctl_file: /etc/sysctl.d/99-haproxy-nonlocal-bind.conf
    reload: yes
    state: present
```

Esto evita errores de HAProxy como `Cannot bind to VIP`, ya que permite iniciar el proceso sin que la IP estÃ© asignada localmente a la interfaz.

3. **Keepalived no requiere HAProxy para iniciar**

```ini
# override.conf
[Unit]
After=haproxy.service
# NO incluye Requires=haproxy.service
```

Esto asegura que Keepalived pueda arrancar independientemente de HAProxy.

4. **VIP solo se usa despuÃ©s de la estabilizaciÃ³n**

   - El uso de la VIP para el kubeconfig solo se hace despuÃ©s de validar que el balanceador HAProxy estÃ© activo.

5. **ConfiguraciÃ³n de HAProxy**

```haproxy
frontend kubernetes_api
    bind 192.168.0.32:6443
    mode tcp
    option tcplog
    default_backend kubernetes_masters

backend kubernetes_masters
    mode tcp
    balance roundrobin
    option tcp-check
    tcp-check connect port 6443
    default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions
    server master-1 10.17.4.21:6443 check
    server master-2 10.17.4.22:6443 check
    server master-3 10.17.4.23:6443 check
```

---

### ğŸ”§ Validaciones Adicionales

1. Verificar si HAProxy permite bind:

```bash
sysctl net.ipv4.ip_nonlocal_bind
```

2. Verificar configuraciÃ³n de HAProxy:

```bash
haproxy -c -f /etc/haproxy/haproxy.cfg
```

3. Verificar si estÃ¡ corriendo:

```bash
sudo systemctl status haproxy
```

---

### ğŸ§ª Estado de los Balanceadores tras el Playbook `install_haproxy_keepalived.yml`

```plaintext
Nodo	IP	FunciÃ³n
loadbalancer1	192.168.0.30	HAProxy master + VIP
loadbalancer2	192.168.0.31	HAProxy backup + VIP
```

---

### ğŸ“¦ Importante sobre HAProxy

- Requiere `net.ipv4.ip_nonlocal_bind = 1` para aceptar conexiones en IPs VIP que no estÃ©n asignadas localmente.
- Se arranca incluso si la VIP no estÃ¡ disponible aÃºn (por diseÃ±o de HA).
- Las configuraciones estÃ¡n correctamente desacopladas gracias al override systemd y `After=haproxy.service`.

---

### âœ… Conclusiones

- Tu diseÃ±o es resiliente, modular y de alta disponibilidad real.
- El clÃºster no depende de las VIPs para arrancar, lo cual rompe el ciclo â€œhuevo-gallinaâ€.
- En caso de falla de cualquier balanceador, los otros asumen sin intervenciÃ³n humana.
- La infraestructura estÃ¡ lista para producciÃ³n y escalamiento.

---

graph TD
A["Internet"] --> B["Cloudflare DNS + HTTPS<br/>(example.com)"]
B --> C["VIP 192.168.0.33<br/>(IngressRoute HTTP/HTTPS)"]
C --> D["HAProxy<br/>(ingress_http / ingress_https)"]
D --> E["Nodos Worker<br/>(10.17.4.24-26)<br/>Puertos 80 / 443"]
E --> F["Servicio Traefik (ClusterIP)<br/>en K3s"]
F --> G["IngressRoute / Ingress<br/>hacia tus apps"]

### ğŸ“¦ InstalaciÃ³n de HAProxy y Keepalived

```bash
sudo ansible-playbook -i inventory/hosts.ini ansible/playbooks/install_haproxy_keepalived_full.yml
```

---

### ğŸ—‘ï¸ DesinstalaciÃ³n de HAProxy y Keepalived

```bash
sudo ansible-playbook -i inventory/hosts.ini ansible/playbooks/uninstall_haproxy_keepalived.yml
```
